{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw9uu1lT9hKJ",
        "outputId": "d8faed8f-5390-4b97-b7b0-6e9d1d4cf3f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-acb5031d0034>:28: FutureWarning: The default fill_method='ffill' in SeriesGroupBy.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
            "  price_df['price_change_percent'] = price_df.groupby('id')['price_off_peak_var'].pct_change()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature engineering completed and saved to 'engineered_features.csv'.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "data_df = pd.read_csv('data.csv')\n",
        "data_df[\"date_activ\"] = pd.to_datetime(data_df[\"date_activ\"], format='%Y-%m-%d')\n",
        "data_df[\"date_end\"] = pd.to_datetime(data_df[\"date_end\"], format='%Y-%m-%d')\n",
        "data_df[\"date_modif_prod\"] = pd.to_datetime(data_df[\"date_modif_prod\"], format='%Y-%m-%d')\n",
        "data_df[\"date_renewal\"] = pd.to_datetime(data_df[\"date_renewal\"], format='%Y-%m-%d')\n",
        "\n",
        "price_df = pd.read_csv('price_data.csv')\n",
        "price_df[\"price_date\"] = pd.to_datetime(price_df[\"price_date\"], format='%Y-%m-%d')\n",
        "\n",
        "# Remove irrelevant columns\n",
        "data_df = data_df.drop(columns=['Unnamed: 0'], errors='ignore')\n",
        "price_df = price_df.drop(columns=['Unnamed: 0'], errors='ignore')\n",
        "\n",
        "# Extract new temporal features\n",
        "price_df['year'] = price_df['price_date'].dt.year\n",
        "price_df['month'] = price_df['price_date'].dt.month\n",
        "price_df['day_of_week'] = price_df['price_date'].dt.dayofweek\n",
        "\n",
        "# Calculate durations\n",
        "data_df['duration_activ_to_end'] = (data_df['date_end'] - data_df['date_activ']).dt.days\n",
        "data_df['days_to_renewal'] = (data_df['date_renewal'] - pd.Timestamp.now()).dt.days\n",
        "\n",
        "# Combine columns to create derived features\n",
        "price_df['price_var_ratio'] = price_df['price_off_peak_var'] / (price_df['price_peak_var'] + 1e-6)\n",
        "price_df['price_change_percent'] = price_df.groupby('id')['price_off_peak_var'].pct_change()\n",
        "\n",
        "# Merge datasets\n",
        "merged_df = pd.merge(data_df, price_df, on='id', how='inner')\n",
        "\n",
        "# Feature engineering on merged dataset\n",
        "merged_df['avg_price'] = (merged_df['price_off_peak_var'] + merged_df['price_peak_var'] + merged_df['price_mid_peak_var']) / 3\n",
        "merged_df['price_category'] = pd.cut(\n",
        "    merged_df['avg_price'], bins=[0, 0.1, 0.2, float('inf')], labels=['Low', 'Medium', 'High']\n",
        ")\n",
        "\n",
        "# Save the transformed dataset\n",
        "merged_df.to_csv('engineered_features.csv', index=False)\n",
        "\n",
        "print(\"Feature engineering completed and saved to 'engineered_features.csv'.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1. Why Did You Choose the Evaluation Metrics?\n",
        "Evaluation metrics should align with the business objective and the nature of the data problem. For churn prediction, typical metrics include:\n",
        "\n",
        "Accuracy: Measures the percentage of correct predictions but may not be sufficient if the dataset is imbalanced.\n",
        "Precision and Recall: Useful for understanding the trade-off between false positives (predicting churn when the customer doesn’t churn) and false negatives (missing customers who churn).\n",
        "F1-Score: Combines precision and recall, especially useful when dealing with imbalanced datasets.\n",
        "ROC-AUC: Evaluates the model's ability to distinguish between classes across thresholds.\n",
        "Justification:\n",
        "If customer churn prediction has class imbalance (e.g., fewer churners), focus on metrics like precision, recall, F1-score, and AUC-ROC to ensure you're addressing the business need effectively.\n",
        "\n",
        "2. Is the Model Performance Satisfactory?\n",
        "To determine whether performance is satisfactory:\n",
        "\n",
        "Compare metrics against a baseline model (e.g., always predicting the majority class).\n",
        "Evaluate metrics like recall if the cost of missing a churner is high.\n",
        "Consider business implications: Does the predicted churn help create actionable insights or drive interventions?\n",
        "Example Explanation:\n",
        "A high recall means we’re capturing most churners, reducing the risk of losing valuable customers.\n",
        "A balanced F1-score ensures we’re not overfitting to either precision or recall.\n",
        "If the ROC-AUC is significantly above 0.5 (e.g., 0.8 or higher), the model is performing better than random guessing.\n",
        "3. Present Your Work Clearly\n",
        "Code Example for Model Evaluation in Colab:\n",
        "python\n",
        "Copy code\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Example: Predictions and ground truth\n",
        "y_true = test_data['churn']  # Replace with actual test labels\n",
        "y_pred = model.predict(X_test)  # Replace with model predictions\n",
        "y_prob = model.predict_proba(X_test)[:, 1]  # Predicted probabilities for ROC-AUC\n",
        "\n",
        "# Evaluation Metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "roc_auc = roc_auc_score(y_true, y_prob)\n",
        "\n",
        "# Print and Explain Metrics\n",
        "print(\"Model Evaluation Metrics:\")\n",
        "print(f\"Accuracy: {accuracy:.2f} - Percentage of correct predictions.\")\n",
        "print(f\"Precision: {precision:.2f} - Percentage of correctly predicted churn cases out of total predicted churn cases.\")\n",
        "print(f\"Recall: {recall:.2f} - Percentage of actual churn cases identified by the model.\")\n",
        "print(f\"F1-Score: {f1:.2f} - Balance between precision and recall.\")\n",
        "print(f\"ROC-AUC: {roc_auc:.2f} - Ability to distinguish between churn and non-churn.\")\n",
        "\n",
        "# Confusion Matrix for Clear Presentation\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Visualize Confusion Matrix (Optional)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "Example for the Report:\n",
        "Metrics Explanation:\n",
        "\"We used recall to ensure most churners are captured, as missing them could result in significant business losses. Precision was used to evaluate the model's ability to avoid false alarms. The ROC-AUC score provides an overall measure of classification performance.\"\n",
        "\n",
        "Performance Justification:\n",
        "\n",
        "\"The model achieves a recall of 0.85, meaning 85% of churn cases are identified. The precision of 0.75 indicates a balanced performance, ensuring that interventions are targeted effectively. An AUC-ROC of 0.88 confirms the model's robust discriminatory power.\"\n",
        "Clear Presentation:\n",
        "Include visualizations (confusion matrix, AUC-ROC curve) to support explanations."
      ],
      "metadata": {
        "id": "UChK1MqNFiRr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}